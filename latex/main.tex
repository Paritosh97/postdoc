\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{appendix}
\usepackage{url}

\title{Scenario Generation for Interactive Urban Environments}
%\author{Paritosh Sharma, Hui-Yin Wu}
\date{May 2025}

\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{pgfplots}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{array}
\usepackage{enumitem}
\usepackage{lscape}
\usepackage{geometry} 
\usepackage{adjustbox}


\begin{document}

\maketitle

\section*{Project members}

\begin{itemize}
    \item Scientific team: Paritosh Sharma, Hui-Yin Wu
\end{itemize}

\section{Context and objectives}

The document highlights the work plan for the the WP4 of the ANR Creative 3D~\footnote{\url{https://project.inria.fr/creattive3d/}} project. The expected outcome of this project is to create a generative model that is capable of creating personalized 3D low vision rehabilitation scenarios for urban traffic scenarios which can be used in Virtual Reality (VR) environments.

\section{Introduction}

Previous works have shown that 3D generative models can be used to create realistic and interactive environments for various applications, including virtual reality (VR) and augmented reality (AR). These can be further used to not only enhance the user experience, but also to provide personalized training and rehabilitation scenarios. In the context of low vision rehabilitation, these models can be particularly useful for simulating urban pedestrian scenarios that are tailored to the specific needs of individuals with visual impairments. However, most generative approaches have focused on self-driving vehicles or general automobile behaviour in urban environments, rather than pedestrian-centric scenarios. This project aims to address this gap by developing a generative model that can create 3D pedestrian urban scenarios with pedestrians as the ego of the scenario.

\section{Related Work}

In this section, we review the existing literature on generative models for urban scenarios, focusing on grounding techniques, generation paradigms, output types, datasets, validation methods, and limitations. The goal is to provide a comprehensive overview of the current state of research in this area and identify gaps that our project aims to address. 

\subsection{Grounding: Input for Generation}

Generative models have been applied to traffic scenario generation, where models synthesize plausible urban environments, pedestrian layouts, or vehicle positions from various types of input. An input in the context of a generative model is any data—such as a prompt, image, or scenario graph—provided to influence the output. Grounding, on the other hand, is the process of linking elements of that input to specific, coherent representations in the generated scenario, such as ensuring the "footpath" appears visually plausible, is correctly positioned on the "road", and respects spatial relationships or physical constraints. Grounding ensures the generated scenario isn't just randomly composed but meaningfully aligned with the intended semantics of the input. The common ways to do grounding are,

\subsubsection{Rules}

Rules can be used to give a prescriptive logic that defines how to generate or modify a scenario. Rule-based systems are necessarily procedural, meaning they follow a set of predefined steps or algorithms to create scenarios. Popular rule-based systems include \textbf{CityEngine}~\cite{parish2001procedural} and \textbf{Infinigen}~\cite{raistrick2023infinite}, which allows users to create complex 3D scenarios by specifying rules for terrain, vegetation, and other elements. Though these systems can generate diverse and complex scenes by applying rules iteratively, they fail to capture the inherent variability and uncertainty present in real-world environments. This can lead to scenes that, while visually coherent, may not accurately reflect the diversity of urban landscapes.

\subsubsection{Constraints}

Constraints define conditions that must be satisfied to get a valid scenario. Scenic~\cite{fremont2019scenic} is a probabilistic programming language that allows users to define constraints for generating scenarios. It uses a declarative approach to specify the properties of the scenario, such as the layout, objects, and their relationships. These can then be rendered using a frontend such as CARLA~\cite{dosovitskiy2017carla} However, it suffers from the same limitations of rule-based systems, as it can only generate scenarios that fit within the defined constraints, potentially missing out on the richness and variability of real-world environments. ScenicNL ~\cite{elmaaroufi2024scenicnl} extends Scenic by allowing users to specify constraints in natural language, making it more accessible to non-coders. However, it still relies on a predefined set of constraints and may not capture the full complexity of urban environments. 

\subsubsection{Neural Network based}

Neural network-based approaches leverage deep learning techniques to learn complex patterns and relationships in data, enabling the generation of scenarios that are more diverse and realistic. These models can learn from large datasets of urban environments, capturing the inherent variability and uncertainty present in real-world scenarios. ChatDyn~\cite{wei2024chatdyn} creates a 3D scenario from a text prompt by first generating a high-level planning and then uses a PedExecuter and VehExecuter to generate pedestrian and vehicle trajectories. 

- chatdyn
- 3D-ScenarioDreamer
- chatscenario
- urbanworld

\subsubsection{Layout}
top-down map-like view

\subsubsection{Images / Videos}
appearance

\subsubsection{Scenario Graphs}
ontology


\textbf{LayoutGPT}~\cite{feng2023layoutgpt} introduces a training-free method for leveraging LLMs in layout-based visual planning by formatting layouts using a CSS-like structure that LLMs can easily interpret. To support in-context learning, \textbf{CLIP}~\cite{radford2021learning} is used to retrieve the most relevant past examples by comparing the similarity between the input prompt and stored text-image pairs.

\subsection{Generation Paradigms}


\subsection{Output}

Generative models can produce a wide range of outputs depending on their architecture and intended application. These outputs vary in structure and fidelity, ranging from low-level pixel images to high-level semantic representations. In the context of road crossing simulation, choosing the appropriate output type is critical, as it determines how easily the generated content can be interpreted, manipulated, or rendered in downstream systems such as virtual reality.

One common output format is the **semantic layout** or **scenario graph**, which captures relationships between entities (e.g., pedestrians, vehicles, sidewalks) in a structured form. Models like \textbf{LayoutGPT}~\cite{feng2023layoutgpt} and \textbf{ScenarioDiffuser}~\cite{yang2023scenariodiffuser} generate such layouts before rendering scenarios. These intermediate representations provide interpretable and editable control, making them especially useful in applications where spatial relationships must be explicitly preserved. Alternatively, some models generate **RGB images** or **videos** directly from prompts or constraints (e.g., \textbf{StoryDALL-E}~\cite{storydalle}), offering visually rich outputs but less flexibility for interaction or simulation. A few works also explore 3D scenario synthesis (e.g., \textbf{3D-FRONT}~\cite{fu20213dfront}) to create spatially accurate environments suitable for embodied AI agents or VR settings.

Ultimately, the choice of output type influences the realism, controllability, and compatibility of generated scenarios. For road crossing simulations in VR, outputs that preserve semantic and spatial coherence—such as structured layouts or 3D scenario graphs—are more suitable than raw pixel-based generations, which may lack the granularity required for interaction or behavior modeling.


3D scenario generation have been used to generate both indoor and outdoor scenarios.


\subsubsection{Outdoor Scenario Generation}

Outdoor 3D scenario generation techniques have been mostly used to model natural landscapes, urban settings, and other external environments for applications such as autonomous driving simulations.

\subsection{4D Scenario Generation}

4D scenario generation techniques are also capable to generate temporal changes (animation key frames). Existing systems can be classified into 3 categroies.

\paragraph{Purely Constraint based} eg. scenic

\paragraph{Multi-layered Perceptron based} eg. DNF

\paragraph{Hexplane based} eg. DreamGaussian4D

\subsection{Datasets}

In this section, we review the datasets that are used to train the generative models for road crossing scenarios. 



Creative3D, GTASynth etc.
kitti
waymo
agroverse
nuscenarios
sind

\subsection{Validation}

\subsection{Limitations and Open Questions}


\section{Work Plan}





\section*{Appendix}

\section{Meeting notes}

\end{document}