\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{appendix}
\usepackage{url}

\title{Scenario Generation for Interactive Urban Environments}
%\author{Paritosh Sharma, Hui-Yin Wu}
\date{May 2025}

\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{pgfplots}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{array}
\usepackage{enumitem}
\usepackage{lscape}
\usepackage{geometry} 
\usepackage{adjustbox}


\begin{document}

\maketitle

\section*{Project members}

\begin{itemize}
    \item Scientific team: Paritosh Sharma, Hui-Yin Wu
\end{itemize}

\section{Context and objectives}

The document highlights the work plan for the the WP4 of the ANR Creative 3D~\footnote{\url{https://project.inria.fr/creattive3d/}} project. The expected outcome of this project is to create a generative model that is capable of creating personalized 3D low vision rehabilitation scenarios for urban traffic scenarios which can be used in Virtual Reality (VR) environments.

\section{Introduction}

Previous works have shown that 3D generative models can be used to create realistic and interactive environments for various applications, including virtual reality (VR) and augmented reality (AR). These can be further used to not only enhance the user experience, but also to provide personalized training and rehabilitation scenarios. In the context of low vision rehabilitation, these models can be particularly useful for simulating urban pedestrian scenarios that are tailored to the specific needs of individuals with visual impairments. However, most generative approaches have focused on self-driving vehicles or general automobile behaviour in urban environments, rather than pedestrian-centric scenarios. This project aims to address this gap by developing a generative model that can create 3D pedestrian urban scenarios with pedestrians as the ego of the scenario.

\section{Related Work}

In this section, we review the existing literature on generative models for urban scenarios, focusing on grounding techniques, generation paradigms, output types, datasets, validation methods, and limitations. The goal is to provide a comprehensive overview of the current state of research in this area and identify gaps that our project aims to address. 

\subsection{Grounding: Input for Generation}

Generative models have been applied to traffic scenario generation, where models synthesize plausible urban environments, pedestrian layouts, or vehicle positions from various types of input. An input in the context of a generative model is any data—such as a prompt, image, or scenario graph—provided to influence the output. Grounding, on the other hand, is the process of linking elements of that input to specific, coherent representations in the generated scenario, such as ensuring the "footpath" appears visually plausible, is correctly positioned on the "road", and respects spatial relationships or physical constraints. Grounding ensures the generated scenario isn't just randomly composed but meaningfully aligned with the intended semantics of the input. The common ways to do grounding are,

\subsubsection{Rules and Constraints}

\paragraph{Rules} can be used to give a prescriptive logic that defines how to generate or modify a scenario. Rule-based systems are necessarily procedural, meaning they follow a set of predefined steps or algorithms to create scenarios. Table \ref{tab:rule_based_models} lists some of the recent rule-based models for scenario generation. Although these systems allow users to define rules for generating scenarios—such as the placement of buildings, roads, and other elements—they often require an intermediate representation which captures the real-world environments. Creating such representations can be challenging, as it requires a deep understanding of the underlying rules and relationships between different elements. Additionally, rule-based systems can be limited in their ability to generate diverse and realistic scenarios, as they rely on predefined rules that may not capture the full complexity of real-world environments.

\begin{table}[ht]
\centering
    \begin{tabular}{|c|c|c|}
    \hline
    \textbf{Model} & \textbf{Technique} & \textbf{Output} \\ \hline
    CityEngine~\cite{parish2001procedural} & Procedural Modeling with Rules & 3D scene \\ \hline
    Infinigen~\cite{raistrick2023infinite} & Procedural Modeling with Rules & 3D scene \\ \hline
    MetaUrban~\cite{wu2024metaurban} & Description Scripts for Scene Layout & 3D Scenario \\ \hline    
    \end{tabular}
\caption{Rule-based Models for Urban Scenario Generation}
\label{tab:rule_based_models}
\end{table}

\paragraph{Constraints} define conditions that must be satisfied to get a valid scenario. Scenic~\cite{fremont2019scenic} is a probabilistic programming language that allows users to define constraints for generating scenarios. It uses a declarative approach to specify the properties of the scenario, such as the layout, objects, and their relationships. These can then be rendered using a frontend such as CARLA~\cite{dosovitskiy2017carla}. However, it suffers from the same limitations of rule-based systems, as it can only generate scenarios that fit within the defined constraints, potentially missing out on the richness and variability of real-world environments.

\subsubsection{Prompts}

Prompts are textual cues provided to the generative model to guide the scenario creation process. Since the popularity of large language models (LLMs) like GPT-3, prompts have become a common way to interact with generative models. They can be used to specify the desired characteristics of the scenario, such as the type of environment, objects, and their relationships. Table \ref{tab:prompt_based_models} lists some of the recent prompt-based models for scenario generation.

\begin{table}[ht]
\centering
    \begin{tabular}{|c|c|c|}
    \hline
    \textbf{Model} & \textbf{Technique} & \textbf{Output} \\ \hline
    ScenicNL~\cite{elmaaroufi2024scenicnl} & Converts LLM Prompts to Scenic Constraints & Scenic Scenario \\ \hline
    ChatScene~\cite{zhang2024chatscene} & Conversational Agent for Scenario Definition using Scenic & Scenic Scenario \\ \hline
    LayoutGPT~\cite{feng2023layoutgpt} & Prompts converted to CSS-like Layout Formatting by LLMs & Layout Representation \\ \hline
    ChatDyn~\cite{wei2024chatdyn} & LLM-based layout planning and low-level trajectory generation for Pedestrian and Vehicle & 3D Scenario \\ \hline
    Work by Feng et al.~\cite{feng2025text} & JSON to describe layout and 3D models & 3D Scene \\ \hline
    TTSG~\cite{ruan2024traffic} & LLM-based planning and retrieval & 3D Scenario \\ \hline
    3D-SceneDreamer~\cite{zhang20243d} & Prompt to Image followed by Image to 3D & 3D Scene \\ \hline
    \end{tabular}
\caption{Prompt-based Models for Urban Scenario Generation}
\label{tab:prompt_based_models}
\end{table}

\subsubsection{Layouts}

Layouts are structured representations of scenarios that capture the spatial relationships between different elements, such as buildings, roads, and pedestrians. These may include bird's-eye views, top-down maps, or other forms of spatial representations that provide a high-level overview of the scenario. Table \ref{tab:layout_based_models} lists some of the recent layout-based models for scenario generation.

\begin{table}[ht]
\centering
    \begin{tabular}{|c|c|c|}
    \hline
    \textbf{Model} & \textbf{Technique} & \textbf{Output} \\ \hline
    CC3D~\cite{bahmani2023cc3d} & 2D Layout-based 3D Scene Generation & 3D Scene \\ \hline
    CityDreamer4D~\cite{xie2025citydreamer4d} & Uses a Layout Generator and a traffic scenario generator & 3D Scenario \\ \hline
    \end{tabular}
\caption{Layout-based Models for Urban Scenario Generation}
\label{tab:layout_based_models}
\end{table}


\subsubsection{Multimodal Input}

Multimodal inputs combine different types of data, such as text, images, and structured representations, to provide a richer context for scenario generation. Table \ref{tab:multimodal_models} lists some of the recent multimodal models for scenario generation.

\begin{table}[ht]
\centering
    \begin{tabular}{|c|c|c|}
    \hline
    \textbf{Model} & \textbf{Input Type} & \textbf{Output} \\ \hline
    CityX~\cite{zhang2024cityx} & Prompt, OSM file or Semantic Map & 3D Scenario \\ \hline
    UrbanWorld~\cite{shang2024urbanworld} & Layout data, text prompts, images & 3D Scenario \\ \hline
    CityCraft~\cite{deng2024citycraft} & Prayout data and text prompts & 3D Scene \\ \hline
    Work by Liu et al.~\cite{liu2025controllable} & Scenegraph assisted using text prompts & 3D Scene \\ \hline
    Dynamic City~\cite{bian2024dynamiccity} & Commands, Layouts, Object Inpainting and Trajectory & 3D Scenario \\ \hline
    GAUDI~\cite{bautista2022gaudi} & Conditioning using prompts or Images & 3D Scene \\ \hline
    \end{tabular}
\caption{Multimodal Models for Urban Scenario Generation}
\label{tab:multimodal_models}


\subsubsection{Images / Videos}
appearance

\subsubsection{Scenario Graphs}
ontology

\subsection{Generation Paradigms}

Generative models can be broadly classified into two paradigms: **rule-based** and **data-driven**. Rule-based approaches rely on predefined rules or constraints to generate scenarios, while data-driven methods leverage machine learning techniques to learn from existing data and generate new scenarios b1ased on learned patterns.

\subsubsection{Rule-based Approaches}

\subsection{Data-driven Approaches}

Data-driven  approaches leverage deep learning techniques to learn complex patterns and relationships in data, enabling the generation of scenarios that are more diverse and realistic. These models can learn from large datasets of urban environments, capturing the inherent variability and uncertainty present in real-world scenarios. We can further classify these models into the following categories:

\paragraph{Large Language Model (LLM) based} models use the knowledge learnt from large text corpora to generate scenarios based on natural language prompts. Table \ref{tab:llm_based_models} lists some of the recent LLM-based models for scenario generation.

\begin{table}[ht]
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        \textbf{Model} & \textbf{Technique} & \textbf{Output} \\ \hline        
        LayoutGPT~\cite{feng2023layoutgpt} & CSS-like Layout Formatting for LLMs & Layout Representation \\ \hline
        ChatDyn~\cite{wei2024chatdyn} & LLM-based layout planning and low-level trajectory generation for Pedestrian and Vehicle & 3D Scenario \\ 

    \end{tabular}
    \caption{LLM-based Models for Urban Scenario Generation}
    \label{tab:llm_based_models}
\end{table}

\subsection{Output}

Generative models can produce a wide range of outputs depending on their architecture and intended application. These outputs vary in structure and fidelity, ranging from low-level pixel images to high-level semantic representations. In the context of road crossing simulation, choosing the appropriate output type is critical, as it determines how easily the generated content can be interpreted, manipulated, or rendered in downstream systems such as virtual reality.

One common output format is the **semantic layout** or **scenario graph**, which captures relationships between entities (e.g., pedestrians, vehicles, sidewalks) in a structured form. Models like \textbf{LayoutGPT}~\cite{feng2023layoutgpt} and \textbf{ScenarioDiffuser}~\cite{yang2023scenariodiffuser} generate such layouts before rendering scenarios. These intermediate representations provide interpretable and editable control, making them especially useful in applications where spatial relationships must be explicitly preserved. Alternatively, some models generate **RGB images** or **videos** directly from prompts or constraints (e.g., \textbf{StoryDALL-E}~\cite{storydalle}), offering visually rich outputs but less flexibility for interaction or simulation. A few works also explore 3D scenario synthesis (e.g., \textbf{3D-FRONT}~\cite{fu20213dfront}) to create spatially accurate environments suitable for embodied AI agents or VR settings.

Ultimately, the choice of output type influences the realism, controllability, and compatibility of generated scenarios. For road crossing simulations in VR, outputs that preserve semantic and spatial coherence—such as structured layouts or 3D scenario graphs—are more suitable than raw pixel-based generations, which may lack the granularity required for interaction or behavior modeling.


3D scenario generation have been used to generate both indoor and outdoor scenarios.


\subsubsection{Outdoor Scenario Generation}

Outdoor 3D scenario generation techniques have been mostly used to model natural landscapes, urban settings, and other external environments for applications such as autonomous driving simulations.

\subsection{4D Scenario Generation}

4D scenario generation techniques are also capable to generate temporal changes (animation key frames). Existing systems can be classified into 3 categroies.

\paragraph{Purely Constraint based} eg. scenic

\paragraph{Multi-layered Perceptron based} eg. DNF

\paragraph{Hexplane based} eg. DreamGaussian4D

\subsection{Datasets}

In this section, we review the datasets that are used to train the generative models for road crossing scenarios. 



Creative3D, GTASynth etc.
kitti
waymo
agroverse
nuscenarios
sind

\subsection{Validation}

\subsection{Limitations and Open Questions}


\section{Work Plan}





\section*{Appendix}

\section{Meeting notes}

\end{document}