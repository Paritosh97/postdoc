\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{appendix}
\usepackage{url}
\usepackage{amssymb}    % for \checkmark
\usepackage{booktabs}  

\title{Scenario Generation for Interactive Urban Environments}
%\author{Paritosh Sharma, Hui-Yin Wu}
\date{July 2025}

\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{pgfplots}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{array}
\usepackage{enumitem}
\usepackage{lscape}
\usepackage{geometry} 
\usepackage{adjustbox}


\begin{document}

\maketitle

\section*{Project members}

\begin{itemize}
    \item Scientific team: Paritosh Sharma, Hui-Yin Wu
\end{itemize}

\section{Context and objectives}

The document highlights the work plan for the the WP4 of the ANR Creative 3D~\footnote{\url{https://project.inria.fr/creattive3d/}} project. The expected outcome of this project is to create a generative model that is capable of creating personalized 3D low vision rehabilitation scenarios for urban traffic scenarios which can be used in Virtual Reality (VR) environments.

\section{Introduction}

Previous works have shown that 3D generative models can be used to create realistic and interactive environments for various applications, including virtual reality (VR) and augmented reality (AR). These can be further used to not only enhance the user experience, but also to provide personalized training and rehabilitation scenarios. In the context of low vision rehabilitation, these models can be particularly useful for simulating urban pedestrian scenarios that are tailored to the specific needs of individuals with visual impairments. However, most generative approaches have focused on self-driving vehicles or general automobile behaviour in urban environments, rather than pedestrian-centric scenarios. This project aims to address this gap by developing a generative model that can create 3D pedestrian urban scenarios with pedestrians as the ego of the scenario.

\section{Related Work}

In this section, we review the existing literature on generative models for urban scenarios, focusing on grounding techniques, generation paradigms, output types, datasets, validation methods, and limitations. The goal is to provide a comprehensive overview of the current state of research in this area and identify gaps that our project aims to address. 

\subsection{Grounding: Input for Generation}

Generative models have been applied to traffic scenario generation, where models synthesize plausible urban environments, pedestrian layouts, or vehicle positions from various types of input. An input in the context of a generative model is any data—such as a prompt, image, or scenario graph—provided to influence the output. Grounding, on the other hand, is the process of linking elements of that input to specific, coherent representations in the generated scenario, such as ensuring the "footpath" appears visually plausible, is correctly positioned on the "road", and respects spatial relationships or physical constraints. Grounding ensures the generated scenario isn't just randomly composed but meaningfully aligned with the intended semantics of the input. The common ways to do grounding are,

\subsubsection{Rules and Constraints}

\paragraph{Rules} can be used to give a prescriptive logic that defines how to generate or modify a scenario. Rule-based systems are necessarily procedural, meaning they follow a set of predefined steps or algorithms to create scenarios. Table \ref{tab:rule_based_models} lists some of the recent rule-based models for scenario generation. Although these systems allow users to define rules for generating scenarios—such as the placement of buildings, roads, and other elements—they often require an intermediate representation which captures the real-world environments. Creating such representations can be challenging, as it requires a deep understanding of the underlying rules and relationships between different elements. Additionally, rule-based systems can be limited in their ability to generate diverse and realistic scenarios, as they rely on predefined rules that may not capture the full complexity of real-world environments.

\begin{table}[ht]
\centering
    \begin{tabular}{|c|c|c|}
    \hline
    \textbf{Model} & \textbf{Technique} & \textbf{Output} \\ \hline
    CityEngine~\cite{parish2001procedural} & Procedural Modeling with Rules & 3D scene \\ \hline
    Infinigen~\cite{raistrick2023infinite} & Procedural Modeling with Rules & 3D scene \\ \hline
    MetaUrban~\cite{wu2024metaurban} & Description Scripts for Scene Layout & 3D Scenario \\ \hline    
    \end{tabular}
\caption{Rule-based Models for Urban Scenario Generation}
\label{tab:rule_based_models}
\end{table}

\paragraph{Constraints} define conditions that must be satisfied to get a valid scenario. Scenic~\cite{fremont2019scenic} is a probabilistic programming language that allows users to define constraints for generating scenarios. It uses a declarative approach to specify the properties of the scenario, such as the layout, objects, and their relationships. These can then be rendered using a frontend such as CARLA~\cite{dosovitskiy2017carla}. However, it suffers from the same limitations of rule-based systems, as it can only generate scenarios that fit within the defined constraints, potentially missing out on the richness and variability of real-world environments.

\subsubsection{Prompts}

Prompts are textual cues provided to the generative model to guide the scenario creation process. Since the popularity of large language models (LLMs) like GPT-3, prompts have become a common way to interact with generative models. They can be used to specify the desired characteristics of the scenario, such as the type of environment, objects, and their relationships. Table \ref{tab:prompt_based_models} lists some of the recent prompt-based models for scenario generation.

\begin{table}[h]
\centering
\resizebox{\textwidth}{!}{
    \begin{tabular}{|c|c|c|}
    \hline
    \textbf{Model} & \textbf{Technique} & \textbf{Output} \\ \hline
    ScenicNL~\cite{elmaaroufi2024scenicnl} & Converts LLM Prompts to Scenic Constraints & Scenic Scenario \\ \hline
    ChatScene~\cite{zhang2024chatscene} & Conversational Agent for Scenario Definition using Scenic & Scenic Scenario \\ \hline
    LayoutGPT~\cite{feng2023layoutgpt} & Prompts converted to CSS-like Layout Formatting by LLMs & Layout Representation \\ \hline
    ChatDyn~\cite{wei2024chatdyn} & LLM-based layout planning and low-level trajectory generation for Pedestrian and Vehicle & 3D Scenario \\ \hline
    Work by Feng et al.~\cite{feng2025text} & JSON to describe layout and 3D models & 3D Scene \\ \hline
    TTSG~\cite{ruan2024traffic} & LLM-based planning and retrieval & 3D Scenario \\ \hline
    3D-SceneDreamer~\cite{zhang20243d} & Prompt to Image followed by Image to 3D & 3D Scene \\ \hline
    Work by Liu et al.~\cite{liu2024graph} & Uses LLM to create a scene graph and then a 3D scene & 3D Scene \\ \hline
    Scenethesis~\cite{ling2025scenethesis} & Uses LLM to create a scene graph and then a 3D scene & 3D Scene \\ \hline
    SceneX~\cite{zhou2024scenex} & LLM to plan PCG (Procedural Controllable Generation) & 3D Scene \\ \hline
    Surreal Drivers~\cite{jin2024surrealdriver} & Chain-of-thought prompts & 3D Scene \\ \hline
    Work by Feng et al.~\cite{feng2025text} & Text prompts & 3D Scene \\ \hline
    Text2nerf~\cite{zhang2024text2nerf} & Text prompts & 3D Scene \\ \hline
    X-Scene~\cite{yang2025x} & Text prompts & 3D Scene \\ \hline
    \end{tabular}
}
\caption{Prompt-based Models for Urban Scenario Generation}
\label{tab:prompt_based_models}
\end{table}


\subsubsection{Layouts}

Layouts are structured representations of scenarios that capture the spatial relationships between different elements, such as buildings, roads, and pedestrians. These may include bird's-eye views, top-down maps, or other forms of spatial representations that provide a high-level overview of the scenario. Table \ref{tab:layout_based_models} lists some of the recent layout-based models for scenario generation.

\begin{table}[ht]
\centering
    \begin{tabular}{|c|c|c|}
    \hline
    \textbf{Model} & \textbf{Technique} & \textbf{Output} \\ \hline
    CC3D~\cite{bahmani2023cc3d} & 2D Layout-based 3D Scene Generation & 3D Scene \\ \hline
    CityDreamer4D~\cite{xie2025citydreamer4d} & Uses a Layout Generator and a traffic scenario generator & 3D Scenario \\ \hline
    Infinicube~\cite{lu2024infinicube} & Text prompts, HD Maps and Bounding Boxes & 3D Scenario \\ \hline
    Work by Zhang et al.~\cite{urbandiff} & BEV map & 3D Scene \\ \hline
    UniScene~\cite{li2025uniscene} & BEV map & Multi-view video \\ \hline
    Savkin et al.~\cite{savkin2021unsupervised} & Scenegraph & Scenario Image \\ \hline
    \end{tabular}
\caption{Layout-based Models for Urban Scenario Generation}
\label{tab:layout_based_models}
\end{table}

\subsubsection{Multimodal}

Multimodal inputs combine different types of data, such as text, images, and structured representations, to provide a richer context for scenario generation. Table \ref{tab:multimodal_models} lists some of the recent multimodal models for scenario generation.

\begin{table}[ht]
\centering
\resizebox{\textwidth}{!}{
    \begin{tabular}{|c|c|c|}
    \hline
    \textbf{Model} & \textbf{Input Type} & \textbf{Output} \\ \hline
    CityX~\cite{zhang2024cityx} & Prompt, OSM file or Semantic Map & 3D Scenario \\ \hline
    CityCraft~\cite{deng2024citycraft} & Layout data and text prompts & 3D Scene \\ \hline
    Work by Liu et al.~\cite{liu2025controllable} & Scenegraph assisted using text prompts & 3D Scene \\ \hline
    Dynamic City~\cite{bian2024dynamiccity} & Commands, Layouts, Object Inpainting and Trajectory & 3D Scenario \\ \hline
    GAUDI~\cite{bautista2022gaudi} & Conditioning using prompts or Images & 3D Scene \\ \hline
    MagicDrive3D~\cite{gao2024magicdrive3d} & Text prompts, Bird Eye View (BEV) map and 3D Bounding Boxes & Reconstructed 3D video \\ \hline
    Scene123~\cite{yang2024scene123} & Text prompt, Image or Text Description & 3D Scene \\ \hline
    StreetScapes~\cite{deng2024streetscapes} & BEV and height map with support for prompts & Video \\ \hline
    Urban Architect~\cite{lu2024urban} & 3D Layout and Text Prompts & 3D Scene \\ \hline
    Urban World~\cite{shang2024urbanworld} & Layout (generation) and prompts (refinement) & 3D Scenario \\ \hline
    Wonderplay~\cite{li2025wonderplay} & Image and action (physics) & 3D Video \\ \hline
    \end{tabular}}
\caption{Multimodal Models for Urban Scenario Generation}
\label{tab:multimodal_models}
\end{table}

\subsection{Generation Paradigms}

Generative models can be broadly classified into two paradigms: \textbf{procedural} and \textbf{data-driven}. Procedural approaches rely on predefined rules or constraints to generate scenarios, while data-driven methods leverage machine learning techniques to learn from existing data and generate new scenarios based on learned patterns.

\subsubsection{Procedural Approaches}

Procedural approaches use a set of rules or algorithms to generate scenarios.

\subsubsection{Data-driven Approaches}

Data-driven  approaches leverage deep learning techniques to learn complex patterns and relationships in data, enabling the generation of scenarios that are more diverse and realistic. These models can learn from large datasets of urban environments, capturing the inherent variability and uncertainty present in real-world scenarios. We can further classify these models into the following categories:

\paragraph{GANs} 

\paragraph{Autoregressive} 

\paragraph{Diffusion} 


\subsection{Datasets}

In this section, we review the datasets that are used to train generative models for road crossing scenarios. Table~\ref{tab:datasets} lists some of the datasets commonly used for training and evaluating generative models in urban environments.

\begin{table}[ht]
\centering
\caption{Overview of selected datasets for foundation model-based scenario generation and analysis.}
\label{tab:datasets}
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{lccccccc}
\toprule
\textbf{Dataset} & \textbf{Year} & \textbf{View} & \textbf{Source}  & \textbf{Dimensionality} \\
\midrule
SIND~\cite{xu2022drone}             & 2022 & BEV        & --         & -- \\
Waymo Open~\cite{sun2020scalability}       & 2020 & FPV        & Real & 3D  \\
Argoverse~\cite{chang2019argoverse}\cite{wilson2023argoverse}        & 2023 & BEV/FPV    & \checkmark & \checkmark \\
nuScenes~\cite{caesar2020nuscenes}      & 2022 & FPV        & \checkmark & \checkmark \\
KITTI~\cite{geiger2012we}            & 2012 & FPV        & \checkmark & \checkmark \\
Cityscapes~\cite{cordts2016cityscapes}      & 2016 & FPV        & \checkmark & \checkmark \\
SemanticKITTI~\cite{behley2019semantickitti}      & 2019 & FPV        & \checkmark & \checkmark \\
HoliCity~\cite{zhu2021holicity}          & 2020 & FPV        & \checkmark & \checkmark \\
OmniCity~\cite{zhu2022omniscity}          & 2023 & FPV        & \checkmark & \checkmark \\
KITTI-360~\cite{behley2019semantickitti}      & 2023 & FPV        & \checkmark & \checkmark \\
GoogleEarth~\cite{googleearth}          & 2024 & FPV        & \checkmark & \checkmark \\
OSM~\cite{openstreetmap}          & 2024 & BEV        & \checkmark & \checkmark \\
CARLA~\cite{dosovitskiy2017carla}          & 2017 & BEV/FPV    & \checkmark & \checkmark \\
Virtual-KITTI-2~\cite{cabon2020virtual}      & 2020 & FPV        & \checkmark & \checkmark \\
CarlaSC~\cite{carla2021}          & 2022 & BEV/FPV    & \checkmark & \checkmark \\
CityTopia~\cite{zhu2023citytopia}          & 2025 & BEV/FPV    & \checkmark & \checkmark \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Validation}

Table \ref{tab:validation} lists common qualitative and quantitative validation methods used to evaluate the performance of generative models for urban road-crossing scenarios. These methods assess the quality, realism, and diversity of generated scenarios to ensure they fulfill their intended applications and research needs.

\begin{table*}[ht]
\centering
\caption{Qualitative and Quantitative Evaluation Methods for Road-Crossing Scenarios}
\resizebox{\textwidth}{!}{
\begin{tabular}{|p{2.5cm}|p{3.5cm}|p{6.2cm}|p{4.5cm}|}
\hline
\textbf{Method Type} & \textbf{Evaluation Approach} & \textbf{Description} & \textbf{Use Case} \
\hline
\multirow{5}{*}{Qualitative}
& Human Review & Human experts assess realism, scenario diversity, and layout plausibility. & Validates human-perceived quality and applicability of the scene. \
\cline{2-4}
& Scenario Visualization & 3D visual inspection or rendered videos showing pedestrian, vehicle, and environment interactions. & Helps detect unrealistic or unnatural behavior/configurations. \
\cline{2-4}
& Surveys & Collects subjective feedback on realism, perceived difficulty, or stress levels from participants. & Measures human-centric realism or emotional response. \
\cline{2-4}
& Comparison & Compare scenario features (traffic density, gap opportunities, layout) to real-world statistics or distributions. & Validates realism by matching key scenario characteristics to empirical data. \
\cline{2-4}
& Failure Cases & Identification and analysis of implausible or unsafe scenarios generated by the model. & Guides iterative model improvement. \
\hline

\multirow{8}{}{Quantitative}
& Scenario Feature Statistics & Statistical analysis of scenario properties (traffic speed, number/duration of gaps, crosswalk presence, etc.). & Ensures generated scenarios span realistic distributions. \
\cline{2-4}
& Coverage and Diversity Metrics & Measures distributional entropy or spread of key attributes across all generated scenarios. & Assesses generalizability, scenario variety, and model's exploration of edge cases. \
\cline{2-4}
& Criticality and Opportunity Metrics & Quantifies the frequency and severity of challenging situations (number of safe gaps, minimum feasible gap size, “no-go” cases). & Evaluates risk/challenge spectrum in the scenario catalog. \
\cline{2-4}
& Sim2Real Gap (Domain Distance) & Computes metrics like KL-divergence, Earth Mover's Distance, t-SNE or FID between generated and real scenario feature distributions. & Evaluates how closely synthetic scenarios match reality. \
\hline
\end{tabular}}
\label{tab:validation}
\end{table}

- Forecasting ? (AgentFormer)
- XDE for agents?

\section{Code}

Currently tested models include:

\subsection{Scenic}

Scenic~\cite{fremont2019scenic} is a probabilistic programming language designed for scenario generation. It allows users to define scenarios using constraints and rules, which can then be rendered in environments like CARLA~\cite{dosovitskiy2017carla}. 

\subsection{MagicDrive}

MagicDrive~\cite{gao2023magicdrive} is a framework that combines large language models (LLMs) with 3D scene generation. It uses prompts to guide the generation of urban scenarios, allowing for the creation of diverse and realistic environments. The framework can generate scenarios in CARLA and supports various input modalities, including text prompts and bounding boxes.

\subsection{MetaUrban/ScenarioNET}

MetaUrban~\cite{wu2024metaurban} and ScenarioNET~\cite{li2023scenarionet} are frameworks that focus on generating urban scenarios using a combination of procedural and data-driven approaches. They allow for the creation of complex urban environments with a focus on pedestrian and vehicle interactions.

\subsection{Threestudio}

Threestudio~\cite{liu2023threestudio} is a framework for generating 3D assets and environments. It uses a combination of procedural and data-driven techniques to create realistic 3D models and scenes, which can be used in various applications, including virtual reality and gaming.


\subsection{Problem and Open Questions}

Most existing scenario-generation methods are vehicle-centric, focusing primarily on creating synthetic scenarios to train autonomous driving agents. A few works such as Jaywalkingvr~\cite{mukoya2024jaywalkervr} and Creative3D\cite{wu2023exploring}  use a fixed set of real and synthetic scenarios to explore pedestrian behaviour. They however rely on on a fixed set of scenarios and interactions. Thus, developing a model that generates diverse pedestrian-centric scenarios can be a significant contribution. The  open questions are then:

\begin{itemize}
    \item \textbf{How to generate interactions based on a given ontology:} 

    \item \textbf{How to create a scenario which allows for diverse pedestrian behaviors:} 

    \item \textbf{How to evaluate interactive scenarios:} 
\end{itemize}


\section{Work Plan}



\section*{Appendix}

\section{Misc}

\bibliographystyle{abbrv}
\bibliography{main}

\end{document}