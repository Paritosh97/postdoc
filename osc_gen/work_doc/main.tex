\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{appendix}
\usepackage{url}
\usepackage{amssymb}    % for \checkmark
\usepackage{booktabs}
\usepackage[most]{tcolorbox}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usetikzlibrary{decorations.pathreplacing}
\usetikzlibrary{calc}
\usetikzlibrary{fit,backgrounds,positioning}
\usepackage{adjustbox}    % Add this in your preamble
\usepackage{rotating}
\usepackage{listings}
\usepackage{xcolor}

% Light gray background for code boxes
\definecolor{codebg}{rgb}{0.95,0.95,0.95} % light gray
\definecolor{keywordcolor}{rgb}{0,0,0.6}  % dark blue for keywords
\definecolor{commentcolor}{rgb}{0.13,0.55,0.13} % green for comments
\definecolor{stringcolor}{rgb}{0.63,0.13,0.94} % purple for strings

% Python style
\lstdefinestyle{pythonstyle}{
    backgroundcolor=\color{codebg}, % light background
    frame=single, % box
    rulecolor=\color{black}, 
    basicstyle=\ttfamily\footnotesize, % dark text
    keywordstyle=\color{keywordcolor}\bfseries,
    commentstyle=\color{commentcolor}\itshape,
    stringstyle=\color{stringcolor},
    breaklines=true,
    captionpos=b,
    numbers=left,
    numberstyle=\tiny,
    xleftmargin=10pt,
    xrightmargin=10pt,
}


\title{Scenario Generation for Interactive Urban Environments}
\author{Paritosh Sharma, Hui-Yin Wu}
\date{February 2026}

\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{hyperref}
\usepackage{pgfplots}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{array}
\usepackage{enumitem}
\usepackage{lscape}
\usepackage{geometry}
\usepackage{adjustbox}

\usetikzlibrary{shadows,arrows.meta,positioning,fit}

\tikzset{
  module/.style={rectangle, rounded corners=6pt, draw=orange!80!black, very thick, fill=black!90,
                 text=white, align=left, font=\bfseries, minimum width=3.5cm, minimum height=2cm},
  subtext/.style={font=\scriptsize\color{white}, align=left},
  arrow/.style={-{Latex[length=3mm]}, thick, orange!80!black},
}

\tcbuselibrary{listings, breakable}

% Define a style for examples
\tcbset{
  examplebox/.style={
    colback=blue!3!white,
    colframe=blue!60!black,
    coltitle=black,
    fonttitle=\bfseries,
    sharp corners,
    boxrule=0.6pt,
    breakable,
    enhanced,
    left=5pt, right=5pt, top=5pt, bottom=5pt
  }
}

\begin{document}

\maketitle

\section*{Project members}

\begin{itemize}
    \item Scientific team: Paritosh Sharma, Hui-Yin Wu
\end{itemize}

\section{Context and objectives}

The document highlights the work plan for the the WP4 of the ANR Creative 3D~\footnote{\url{https://project.inria.fr/creattive3d/}} project. The expected outcome of this project is to create a generative model that is capable of creating personalized training scenarios in urban environments.

\section{Introduction}

Virtual Reality (VR) and Augmented Reality (AR) technologies have advanced significantly in recent years, enabling the creation of immersive and interactive environments for various applications. These can be further used to provide personalized training and rehabilitation scenarios. In the context of low vision rehabilitation, these models can be particularly useful to study pedestrian behaviours under normal and simulated vision. However, most simulated environments suffer from perceptual gaps between the designer, the user, and the system. The existing GUsT-3D framework~\cite{wu2022designing}, developed during the Creative3D project, provides a foundation to address this. However, the current framework is still limited by its reliance on a fixed set of urban environments and interactions innhibiting its ability to scale.

In parallel, recent works on 3D generative models for urban scenario generation has shown promising results in generating diverse and realistic urban environments. Additionally, these models have been able to capture the diverse nature and the complexities of an urban setting, including the interactions between pedestrians, vehicles, and the environment. Driven by these extraordinary capabilities, exploring the potential of generative models will enable us to create more diverse and realistic urban scenarios.

\section{Related Work}
\label{sota}

In this section, we first review prior work on pedestrian-in-the-loop simulation. 
We then survey recent generative approaches for urban scenario generation, covering symbolic, layout-guided, and neural rendering–based methods, as well as diffusion-based models for dynamic scene synthesis. 
Finally, we discuss common validation protocols used to evaluate the performance of these models.


\subsection{Pedestrian-in-the-Loop}
\label{sota:ped_in_loop}

VR Simulations have been widely used to study pedestrian behavior and interactions in urban scenarios~\cite{wu2018using,tran2021review,mukoya2024jaywalkervr,schneider2020virtually}. This human-in-the-loop approach (also referred as pedestrian-in-the-loop\cite{hartmann2017pedestrian}) allows researchers to collect data on how pedestrians interact with their environment, including their decision-making processes, movement patterns, and responses to various stimuli. These simulations can be used to study a wide range of scenarios, from simple road crossings to complex urban environments with multiple interacting agents. More recently, JaywalkerVR~\cite{mukoya2024jaywalkervr}, a VR human-in-the-loop simulator, used CARLA~\cite{dosovitskiy2017carla} to create four different scenarios: jaywalking, parked cars, four-way stops, and parking lot entrances. Despite the obvious advantages, developing such simulators is still challenging because of the perceptual gaps between the designer, the user and the system as identified by Dourish~\cite{dourish2001action}.

The GUsT-3D framework~\cite{wu2022designing} addresses this by first defining the scene using a scenegraph, which captures the relationships between different elements in the scenario (ontology) and then defining the task to be carried out during the course of the scenario using a GUTasks (intersubjectivity). Lastly, it uses a query component for logging and post-scenario analysis of the experience (intentionality). This framework was also applied by creating a dataset of 6 road-crossing scenarios to study pedestrian behavior under normal and low vision~\cite{wu2023exploring}. Even though GusT-3D framework addresses the perceptual gaps which were identified by Dourish, it still relies on a fixed set of scenarios and interactions.

\subsection{Scenario Generation}
\label{sota:procedural}

\subsubsection{Structured Scenario Generation}
\label{sota:generative_models:structured}

Structured scenario generation refers to methods that produce scenarios in formal, machine-readable representations suitable for execution in simulators and systematic evaluation.

The most widely used formats in autonomous driving research include:

\begin{itemize}
    \item \textbf{Scenic}~\cite{fremont2019scenic}: a probabilistic domain-specific language for defining constraints, distributions, and relationships among agents. Scenarios specified in Scenic can be sampled into concrete instances.
    
    \item \textbf{OpenSCENARIO DSL}~\cite{ASAMOpenSCENARIODSL} is an open standard for describing dynamic scenarios in driving and traffic simulators. Scenarios can be defined at four levels—concrete, logical, abstract, and functional~\cite{ASAMOpenSCENARIO}—and are typically composed of three main components: \textbf{Entities} (vehicles, pedestrians, traffic signs, obstacles), \textbf{Road Network} (the road layout and topology), and \textbf{Storyboard} (containing Story, which further defines Actions, including maneuvers, trajectories, and decision logic). Figure~\ref{app:fig:openscenario_tree} illustrates this hierarchical structure, which provides a systematic framework for defining and simulating urban scenarios.

    
    \item \textbf{GeoScenario DSL}~\cite{8814107}: similar to OpenSCENARIO, but provides concrete, executable scenario specifications with explicit positions, trajectories, and temporal details, making scenarios directly runnable and reusable across different simulation platforms.
    
    \item \textbf{ScenarioNet}~\cite{li2023scenarionet}: built on real-world data for structured, executable traffic scenarios.
\end{itemize}

\begin{figure}[h]
    \centering
    \includegraphics[width=.6\textwidth]{images/qual_scenario_descr.png}
    \caption{OpenScenario Levels of Abstraction~\cite{ASAMOpenSCENARIO}}
    \label{fig:openscenario_levels}
\end{figure}


\subsubsection{Prompt-based Procedural Specification}
\label{sota:generative_models:prompts}

Prompts are textual cues provided to the generative model to guide the scenario creation process. Since the popularity of LLMs like GPT~\cite{achiam2023gpt}, prompts have become a common way to interact with generative models. They can be used to specify the desired characteristics of the scenario, such as the type of environment, objects, and their relationships. Table~\ref{tab:prompt_based_models} lists some of the recent prompt-based models for scenario generation.

\begin{table}[h]
\centering
\resizebox{\textwidth}{!}{
    \begin{tabular}{|c|c|c|}
    \hline
    \textbf{Model} & \textbf{Technique} & \textbf{Output} \\ \hline
    ScenicNL~\cite{elmaaroufi2024scenicnl} & Converts LLM Prompts to Scenic Constraints & Scenic Scenario \\ \hline
    ChatScene~\cite{zhang2024chatscene} & Conversational Agent for Scenario Definition using Scenic & Scenic Scenario \\ \hline
    LayoutGPT~\cite{feng2023layoutgpt} & Prompts converted to CSS-like Layout Formatting by LLMs & Layout Representation \\ \hline
    ChatDyn~\cite{wei2024chatdyn} & LLM-based planning and low-level trajectory generation for Pedestrian and Vehicle & 3D Scenario \\ \hline
    Work by Feng et al.~\cite{feng2025text} & JSON to describe layout and 3D models & 3D Scene \\ \hline
    TTSG~\cite{ruan2024traffic} & LLM-based planning and retrieval & 3D Scenario \\ \hline
    Scenethesis~\cite{ling2025scenethesis} & Uses LLM to create a scenegraph and then a 3D scene & 3D Scene \\ \hline
    SceneX~\cite{zhou2024scenex} & LLM to plan PCG (Procedural Controllable Generation) & 3D Scene \\ \hline
    \end{tabular}
}
\caption{Prompt-based Models for Urban Scenario Generation}
\label{tab:prompt_based_models}
\end{table}

\subsubsection{Multimodal Procedural Specification}
\label{sota:generative_models:multimodal}

Some works have also combined multimodal inputs with different types of data, prompts, layouts and other structured representations, to provide a richer context for scenario generation. Table~\ref{tab:multimodal_models} lists some of the recent multimodal models for urban scenario generation.

\begin{table}[ht]
\centering
\resizebox{\textwidth}{!}{
    \begin{tabular}{|c|c|c|}
    \hline
    \textbf{Model} & \textbf{Input Type} & \textbf{Output} \\ \hline
    CityX~\cite{zhang2024cityx} & Prompt, OSM file or Semantic Map & 3D Scenario \\ \hline
    CityCraft~\cite{deng2024citycraft} & Layout data and text prompts & 3D Scene \\ \hline
    \end{tabular}}
\caption{Multimodal Models for Urban Scenario Generation}
\label{tab:multimodal_models}
\end{table}

\section{Problem and Open Question}

Recent generative models have demonstrated strong capabilities in synthesizing urban scenarios, yet they remain limited in modeling pedestrian dynamics within these environments. Specifically, they struggle to generate a pedestrian agent's actions based on a functional description of a scenario.

This motivates the following open research question:

\textbf{How can large language models be leveraged to generate urban scenarios for pedestrian agents using functional scenario descriptions?}

\section{Method}



\subsection{Pedestrian Navigation Task Taxonomy}
\label{sec:task_taxonomy}

\subsubsection{Scope and Inclusion Criteria}

The
task description in the scenario specifies navigation objectives, constraints, triggers, and
evaluation conditions, while execution and decision-making are performed
by the user.

We define a pedestrian navigation task as an atomic interaction unit
that satisfies the following criteria:

\begin{itemize}
    \item \textbf{Intentionality}: the task encodes a clear navigation
    objective or decision the user must perform.
    \item \textbf{Environmental grounding}: the task is defined with
    respect to explicit spatial, traffic, or semantic elements of the
    environment.
    \item \textbf{Evaluability}: the task admits objective success and
    failure conditions observable by the simulator.
    \item \textbf{Composability}: the task can be combined sequentially
    or hierarchically with other tasks to form longer scenarios.
    \item \textbf{Scenario expressibility}: the task can be represented
    declaratively using OpenSCENARIO-style constructs (e.g., goals,
    constraints, triggers).
\end{itemize}

Tasks that differ only in low-level motion execution are excluded, as
locomotion is performed by the user and not modeled by the system.

\subsubsection{Task Taxonomy}

Table~\ref{tab:ped_tasks} summarizes the resulting pedestrian navigation
task taxonomy, including task definitions and corresponding success and
failure conditions.

\begin{table}[t]
\centering
\footnotesize
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{1.15}
\begin{tabular}{p{2.4cm} p{3.2cm} p{3.0cm} p{3.0cm}}
\hline
\textbf{Task} &
\textbf{Description} &
\textbf{Success} &
\textbf{Failure} \\
\hline

Point-to-point &
Reach a target location via walkable areas &
Target reached within tolerance &
Restricted area or timeout \\

Crosswalk-based &
Cross road using designated crosswalk &
Crossing completed within crosswalk &
Leaving crosswalk \\

Multi-step routes &
Complete ordered sequence of sub-goals &
All sub-goals completed in order &
Skipped or reordered steps \\

Traffic-aware crossing &
Initiate crossing with safe traffic gap &
Gap above safety threshold &
Unsafe gap selection \\

Constrained path &
Navigate using permitted paths only &
Destination reached compliantly &
Forbidden path usage \\

Environmental constrained &
Avoid static or semantic obstacles &
Target reached without violation &
Restricted zone entry \\

Triggered navigation &
Begin movement after event trigger &
Movement after trigger condition &
Premature or missed trigger \\

Tolerance-based &
Reach target with positional precision &
Within tolerance region &
Outside tolerance bounds \\

Dynamic navigation &
Adapt path to dynamic obstacles &
Successful re-routing &
Deadlock or violation \\

Speed adaptation &
Adapt walking speed to context &
Speed within bounds &
Speed constraint violation \\

\hline
\end{tabular}
\caption{Pedestrian navigation task taxonomy with success and failure conditions.}
\label{tab:ped_tasks}
\end{table}


We based our method on the task description of the existing GUsT-3D framework~\cite{wu2022designing} and extended the existing OpenSCENARIO specification to include pedestrian tasks. Figure~\ref{fig:method_overview} provides an overview of the method.


% ---- figure ----
\begin{figure}[h]
\centering
\resizebox{\textwidth}{!}{%
\begin{tikzpicture}[
    zone/.style={
        rectangle, draw, rounded corners,
        fill=#1!15, inner sep=0.5cm
    },
    regionbox/.style={
        minimum width=6.2cm,
        minimum height=7.2cm
    },
    subblock/.style={
        rectangle, draw, rounded corners,
        minimum width=5cm, minimum height=1.2cm,
        text centered, font=\footnotesize,
        fill=gray!10
    },
    arrow/.style={->, thick, >=stealth},
    node distance=0.8cm
]

% =====================
% 1. INPUT REGION
% =====================
\node[subblock] (osm_input)
    {\includegraphics[width=4cm]{images/osm_example.png}};
\node[anchor=north] at (osm_input.south) {OSM Map};

\node[subblock, below=0.5cm of osm_input.south, anchor=north] (prompt_input)
    {Generate a scenario where...};
\node[anchor=north] at (prompt_input.south) {Prompt};

\begin{scope}[on background layer]
\node[
    zone=blue,
    regionbox,
    fit=(osm_input)(prompt_input)
] (input_zone) {};
\node[anchor=north west] at (input_zone.north west) {\textbf{Input}};
\end{scope}

% =====================
% 2. CONTEXT REGION
% =====================
\node[
    subblock,
    right=1.2cm of input_zone.north east,
    anchor=north west
] (xodr_block)
    {\includegraphics[width=3cm]{images/road_network_example.png}};
\node[anchor=north] at (xodr_block.south)
    {XODR Road Network};

\node[
    subblock,
    below=0.5cm of xodr_block.south,
    anchor=north,
    font=\scriptsize
] (context_block)
    {Road info, intersections...};
\node[anchor=north] at (context_block.south)
    {OSM Context};

\begin{scope}[on background layer]
\node[
    zone=green,
    regionbox,
    fit=(xodr_block)(context_block)
] (context_zone) {};
\node[anchor=north west] at (context_zone.north west)
    {\textbf{Context Extraction}};
\end{scope}

% =====================
% 3. PLANNING REGION
% =====================
\node[
    subblock,
    right=1.2cm of context_zone.north east,
    anchor=north west,
    font=\scriptsize,
    text width=4.6cm,
    align=left
] (stage1)
{\textbf{Stage 1: Scene Setup}

Parse prompt + context; output entities and initial positions};

\node[
    subblock,
    below=0.5cm of stage1.south,
    anchor=north,
    font=\scriptsize,
    text width=4.6cm,
    align=left
] (stage2)
{\textbf{Stage 2: Entity Creation}

Define vehicles, pedestrians, and attributes};

\node[
    subblock,
    below=0.5cm of stage2.south,
    anchor=north,
    font=\scriptsize,
    text width=4.6cm,
    align=left
] (stage3)
{\textbf{Stage 3: Behavior \& Trajectory}

Generate movements, routes, and actions};

\node[
    subblock,
    below=0.5cm of stage3.south,
    anchor=north,
    font=\scriptsize,
    text width=4.6cm,
    align=left
] (stage4)
{\textbf{Stage 4: VR Tasks}

Create VR\_TASK actions};

\begin{scope}[on background layer]
\node[
    zone=orange,
    regionbox,
    fit=(stage1)(stage4)
] (planning_zone) {};
\node[anchor=north west] at (planning_zone.north west)
    {\textbf{Generation}};
\end{scope}

% =====================
% 4. RENDERING REGION (unchanged)
% =====================

\node[
    subblock,
    below=1cm of context_zone.south
] (render_block)
    {Unity / GUST-3D / ESMINI};

\begin{scope}[on background layer]
\node[
    zone=purple,
    fit=(render_block),
    inner sep=0.8cm
] (rendering_zone) {};
\node[anchor=north west] at (rendering_zone.north west)
    {\textbf{Rendering}};
\end{scope}

% =====================
% ARROWS (foreground)
% =====================

\draw[arrow] (osm_input.east) -- ++(1,0) |- (xodr_block.west);
\draw[arrow] (osm_input.east) -- ++(1,0) |- (context_block.west);
\draw[arrow] (prompt_input.east) -- ++(1,0) |- (stage1.west);
\draw[arrow] (xodr_block.east) -- ++(1,0) |- (stage1.west);
\draw[arrow] (context_block.east) -- ++(1,0) |- (stage1.west);

\draw[arrow]
  (stage4.east)
  -- ++(1.2,0)
  |- (stage1.east)
  node[pos=0.1, sloped, font=\scriptsize, yshift=5pt]
  {scenario invalid};

\draw[arrow]
  (stage4.south)
  -- ++(1.2,0)
  |- (render_block.east)
  node[pos=0.3, sloped, font=\scriptsize, yshift=5pt]
  {scenario valid};


\end{tikzpicture}
}
\caption{Overview of the scenario generation pipelin}
\label{fig:method_overview}
\end{figure}


\subsection{Input}


\subsection{Tasks}

The tasks predicted by the LLM are a sequence of guided actions to be performed by the player or agent in the environment. Each task is defined as a structured object, similar to the following JSON format:

\begin{itemize}
    \item \textbf{id:} a unique identifier for the task.

    \item \textbf{goal:} specifies the type of action, the target object, and optionally a key item needed to perform the action.
    \begin{itemize}
        \item \texttt{type} -- one of the following:
        \begin{itemize}
            \item \texttt{get}: locate and pick up an object (target must have layer ``Movable'').
            \item \texttt{interact}: interact with an object (target must have layer ``Interactable'').
            \item \texttt{interactWith}: interact with a specific object using a key item (target has layer ``Interactable'').
            \item \texttt{go}: navigate to a location (target must have layer ``Ground'').
            \item \texttt{place\_in}: place an object inside a container (target has layer ``Container'', requires \texttt{key\_item}).
            \item \texttt{place\_on}: place an object on a support surface (target has layer ``Support'', requires \texttt{key\_item}).
        \end{itemize}
        \item \texttt{target} -- the identifier of the target object, using the GUST annotation format:
        \begin{itemize}
            \item Ground objects (layer ``Ground''): \texttt{objecttype\_x.0\_y.0}, e.g., \texttt{sidewalk\_50.0\_62.0}, \texttt{road\_50.0\_50.0}.
            \item Other objects: \texttt{objecttype\_id}, e.g., \texttt{traffic\_light\_4}, \texttt{table\_6}, \texttt{flower\_pot\_8}.
        \end{itemize}
        \item \texttt{key\_item} -- optional object required to perform the task (null for simple tasks, required for \texttt{place\_in}, \texttt{place\_on}, and \texttt{interactWith} tasks).
    \end{itemize}

    \item \textbf{baselineTime:} time in seconds that a baseline user should take to complete the task (typically 10--30 seconds for simple tasks, 30--60 seconds for complex tasks).

    \item \textbf{targetTime:} maximum time in seconds allowed for the user to complete the task (typically 1.5--2× \texttt{baselineTime}).

    \item \textbf{Help system:} provides progressive assistance to the user at different stages.
    \begin{itemize}
        \item \texttt{baselineHelp}: type of help provided after \texttt{baselineTime} elapses (``text'', ``path'', or ``nothing'').
        \item \texttt{targetHelp}: type of help provided after \texttt{targetTime} elapses (``text'', ``path'', or ``nothing'').
        \item \texttt{failureHelp}: type of help provided after task failure (``text'', ``path'', or ``nothing'').
        \item \texttt{baselineText}: English help text shown after \texttt{baselineTime} (e.g., ``Walk straight along the sidewalk'').
        \item \texttt{targetText}: English help text shown after \texttt{targetTime} (e.g., ``Reach the sidewalk safely'').
        \item \texttt{failureText}: English help text shown after failure (e.g., ``Stay on designated walking areas'').
    \end{itemize}

    \item \textbf{Failure conditions:} defines when and how the task fails.
    \begin{itemize}
        \item \texttt{failure}: type of failure condition (``collision'' or ``none'').
        \item \texttt{failureItem}: object that causes failure when collided with (e.g., ``vehicle'', ``person''; null if \texttt{failure} is ``none'').
    \end{itemize}

    \item \textbf{constraints:} includes task ordering and evaluation criteria.
    \begin{itemize}
        \item \texttt{precedence}: list of task IDs that must be completed before this task can begin.
        \item \texttt{evaluation}: condition that defines successful task completion (e.g., ``on\_sidewalk'', ``item\_picked\_up'').
    \end{itemize}

    \item \textbf{instructions:} natural language instructions for the player in multiple languages.
    \begin{itemize}
        \item \texttt{text\_en}: English instructions (e.g., ``Walk to the sidewalk at position [50, 62]'').
        \item \texttt{text\_fr}: French instructions (e.g., ``Marchez vers le trottoir à la position [50, 62]'').
    \end{itemize}

    \item \textbf{trigger:} specifies the object and function that triggers task completion.
    \begin{itemize}
        \item \texttt{object\_id}: ID of the object from \texttt{static\_layout}.
        \item \texttt{function}: trigger function name (e.g., ``on\_reach'', ``on\_interact'', ``on\_pickup'').
    \end{itemize}
\end{itemize}

The structure is based on the existing GUsT-3D framework~\cite{wu2022designing} and is integrated with the existing system to provide guided task execution with real-time feedback and progressive assistance.

\section{Implementation}

The implementation of the system is divided into two components — the \textbf{Backend (Python)} and the \textbf{Frontend (Unity)} — which together enable interaction between the LLM and the Unity-based scenario generation environment.

\subsection{Backend (Python) Implementation}

The backend is implemented as an HTTP server in Python that interfaces with the LLM to process requests and manage data flow between the model and the Unity client.
It exposes two primary \texttt{POST} endpoints:

\begin{itemize}
    \item \textbf{/get\_3d} – Receives the OSM bounding box coordinates and returns the 3D scene. For generating the OSM scene, we modify the existing \href{https://github.com/vvoovv/blosm}{blosm addon} for Blender to add support for trash, crosswalks, bus stops and traffic lights.
    \item \textbf{/get\_scenario} – Sends the prompt and receives the OpenScenario dynamics as well as the GUTasks from the Qwen3 LLM agent.
\end{itemize}

\subsubsection{Blosm implementation}

The \href{https://github.com/vvoovv/blosm}{blosm addon} for Blender is extended to add support for trash, crosswalks, bus stops and traffic lights. However, the OSM semantics only provide positional information and no orientation information of the semantic classes. Thus, for elements dependent on the road direction, we use algorithm~\ref{alg:alignment} to calculate the orientation of 3D assets.

\begin{algorithm}[H]
\caption{Compute Orientation for Road-Aligned OSM Objects}
\label{alg:alignment}
\begin{algorithmic}[1]
\Require Semantic object $O$ (e.g., crosswalk, bus stop) with position $p_O$  
\Require OSM road network $R$ composed of segments $S_i$ with start and end coordinates
\Ensure Orientation vector $\vec{d}_O$ for the 3D asset

\State Find the nearest road segment $S_n$ to $p_O$
\State Compute the road direction vector $\vec{v}_n$ of $S_n$:
\[
\vec{v}_n = \frac{S_n.\text{end} - S_n.\text{start}}{\|\ S_n.\text{end} - S_n.\text{start} \|\ }
\]

\If{$O$ is a crosswalk}
    \State $\vec{d}_O \gets$ vector perpendicular to $\vec{v}_{road}$
\ElsIf{$O$ is a bus stop or traffic light}
    \State $\vec{d}_O \gets \vec{v}_{road}$ \Comment{aligned parallel to the road}
\Else
    \State $\vec{d}_O \gets \vec{v}_{road}$ \Comment{default: parallel to road}
\EndIf

\State Place 3D asset for object $O$ at position $p_O$ with orientation $\vec{d}_O$
\end{algorithmic}
\end{algorithm}


\subsection{Frontend (Unity) Implementation}

The frontend is developed as a part of the existing GUsT-3D add-on. It communicates with the Python backend via HTTP, sending requests to the defined endpoints and parsing the responses to generate the scenarios within Unity. 

For the OpenScenario interpretation, we modify the \href{https://github.com/esmini/esmini}{esmini addon} and add support for pedestrians.


\section{Experiments}
\label{sec:experiments}

This section presents a log of all experiments conducted to generate the dynamics and tasks with the \textit{LLM}. 

\subsection{Experiment 1}


\section{Evaluation}
\label{evaluation}

\subsection{Quantitative Evaluation}
\label{evaluation:quant}

To assess the fidelity and diversity of the generated results, we use the following metrics.

\subsubsection{Dynamics Interpretation Success Rate}
The dynamics are considered successfully interpretable if the generated OpenScenario XML can be loaded and visualized in Unity.

\subsubsection{Prompt Adherence for Tasks and Dynamics}
Since no ground-truth scenarios are available for open-ended prompts, we evaluate the semantic quality of generated outputs in terms of \textit{prompt adherence}. Prompt adherence measures how faithfully the generated tasks and dynamics satisfy the semantic requirements explicitly specified in the input prompt.

For each prompt, we extract a set of requested semantic classes, including task types, dynamic behaviors, road and environment attributes, agent roles, and quantitative constraints (e.g., speed ranges or traffic density). From the generated OpenSCENARIO XML and GUTasks representation, we then extract the corresponding set of realized semantic classes.

Prompt adherence is evaluated using precision and recall over these semantic classes. Precision measures the extent to which generated elements are relevant to the prompt and penalizes the introduction of hallucinated or unrequested behaviors. Recall measures the completeness of the generation with respect to the prompt and captures whether all requested tasks and dynamics are present in the output.

Formally, let $R$ denote the set of semantic classes requested by the prompt and $G$ the set of semantic classes extracted from the generated output. Prompt precision and recall are defined as:
\[
\text{Precision} = \frac{|R \cap G|}{|G|}, \quad
\text{Recall} = \frac{|R \cap G|}{|R|}
\]

For quantitative attributes, such as speeds or timing constraints, a class is considered correctly generated if the corresponding value lies within a predefined tolerance range.

\subsubsection{GUTasks Interpretation Success Rate}
Generated GUTasks are considered successfully interpretable if GuST-3D and Unity can load and process them.

\subsubsection{Task Feasibility Rate}
Generated tasks are considered feasible if their required navigation and interaction components can be realized within the simulation environment. Specifically, navigation tasks are deemed feasible if a valid path exists for Unity's NavMesh Agent between the specified start and goal states. Interaction tasks are considered feasible if their preconditions, such as spatial distance, visibility, and agent states, can be satisfied during execution.

\subsection{Qualitative Evaluation}
\label{evaluation:qualitative}

For our experiments, we evaluate the generation qualitatively using a fixed set of predefined prompts, as well as through a user study in which participants freely author their own prompts and generate corresponding scenes.

\subsubsection{Prompts}

We assess the pipeline along two complementary axes: prompt specificity and scenario plausibility. This framework is informed by SOTA practices in which prompts are structured to test the model across a spectrum of detail, starting from vague to detailed, structured guidelines (rubric).

\begin{itemize}
    \item \textbf{Prompt specificity:} Measures the model's robustness to variations in instruction detail~\cite{murugadoss2025evaluating, xu2025large}.
    \begin{itemize}
        \item \emph{Vague prompts:} Underspecified instructions for generating urban scenarios, where the model must infer missing details. For example, "Generate an urban scenario with streets and buildings" leaves most design aspects open to the model.

        \item \emph{Criteria Specific prompts:} Prompts that specify the desired properties or evaluation criteria for the urban scenario, without detailing exact layouts or steps. For instance, "Generate an urban scenario that is realistic, has safe traffic flow, and includes diverse building types" guides the model toward satisfying these properties while leaving implementation and arrangement open.

        \item \emph{Detailed prompts (Rubric):} Fully specified instructions with explicit objectives, constraints, and optional step-by-step guidance for urban scene generation. This may include a rubric enumerating required features, such as "Generate an urban scenario with at least three traffic intersections with controlled signals, a sidewalk along the road and a crosswalk every 50 metres, and pedestrian pathways connecting all major areas. Agent arrives from the north, presses on the pedestrian light and then crosses the road when green."
    \end{itemize}

    \item \textbf{Scenario plausibility:} Evaluates the realism, logical coherence, and potential risk of the model-generated tasks~\cite{parmar2024logicbench, zhu2023promptbench}.
    \begin{itemize}
        \item \emph{Illogical scenarios:} Contain contradictory or impossible instructions to probe reasoning limitations . Example, "Create a scenario with no roads with busy traffic. Agent must cross using the crosswalk."
        \item \emph{Realistic scenarios:} Plausible, safe instructions representing typical interactions. Example, "Create a scenario with multiple cars and bicycles. Agent must wait for the pedestrian signal before crossing."
        \item \emph{Critical scenarios:} Edge-case or high-risk tasks revealing latent model limitations. Example, Jaywalking, Parking lot navigation, etc.
        %\item \emph{Prompts reflecting original GUsT-3D scenarios:} Baseline scenarios sourced directly from the dataset for comparison with prior work.
    \end{itemize}
\end{itemize}

\paragraph{Prompt Grid Design}
To systematically explore model behavior, each task is tested using prompts that span the two axes.

\begin{center}
\begin{tabular}{c|c|c|c}
\textbf{Prompt Specificity} & \textbf{Illogical} & \textbf{Realistic} & \textbf{Critical} \\ \hline
\textbf{Vague} & P1 & P2 & P3 \\
\textbf{Detailed} & P4 & P5 & P6
\end{tabular}
\end{center}


\subsubsection{User Studies}
In addition to evaluation using predefined prompts, we conduct a user study to qualitatively assess the behavior of the system under open-ended, user-authored inputs.

\clearpage
\section{Appendix}

\subsection{Original OpenSCENARIO specification}
A brief tree of the OpenSCENARIO specification is show in figure~\ref{app:fig:openscenario_tree}

\subsection{Ontology of CustomCommandAction}

\begin{table}[h!]
\centering
\caption{CustomCommandAction Ontology}
\begin{tabular}{|l|p{5cm}|l|p{6cm}|} 
\hline
\textbf{Field} & \textbf{Type / Options} & \textbf{Required?} & \textbf{Description / Purpose} \\
\hline
id & Integer & Yes & Unique identifier for the task; used to order tasks in the guided action sequence. \\
\hline
goal.type & Enum: get, interact, interactWith, go, place\_in, place\_on & Yes & \texttt{get → Movable, interact/interactWith → Interactable, go → Ground, place\_in → Container, place\_on → Support}. \\
\hline
goal.target & String & Yes & Must reference an actual object type in \texttt{static\_layout} matching the required layer for the task type. \\
\hline
goal.key\_item & String / null & No & Optional item required to complete the task; null if not needed. \\
\hline
constraints.precedence & Integer & No & Defines ordering relative to other tasks; lower numbers execute first. \\
\hline
constraints.evaluation & String (accuracy, completion time, success/fail, custom) & No & Criteria for assessing task completion. \\
\hline
help.baseline & String & No & Guidance message describing standard execution. \\
\hline
help.target & String & No & Guidance message highlighting the goal or object of interest. \\
\hline
help.failure & String & No & Guidance message for failure conditions or corrective action. \\
\hline
failure\_condition.type & Enum: Collision, Timeout, IncorrectTarget, Misstep & No & Type of failure that can terminate or penalize the task. \\
\hline
failure\_condition.item & String & No & Object or condition that triggers failure (e.g., Vehicle, Pedestrian, Door). \\
\hline
instructions.text\_en & String & Yes & Human-readable instructions in English for the pedestrian. \\
\hline
instructions.text\_fr & String & No & Human-readable instructions in French (or other languages). \\
\hline
trigger.object\_id & String & No & Optional object whose state or event triggers the start of this task. \\
\hline
trigger.function & String & No & Function or event of the trigger object that initiates task execution. \\
\hline
\end{tabular}
\end{table}




\subsection{Examples}
\label{app:examples}

The following example represents a scenario where a pedestrian has to cross the street while the vehicle approaches.

% =====================
% Python Code Example
% =====================
\begin{lstlisting}[style=pythonstyle, caption={Python code}, language=Python]
from scenariogeneration import xosc

# -----------------------------
# 1. ENTITIES
# -----------------------------
entities = xosc.Entities()

# Ego pedestrian
ego = xosc.Pedestrian(
    name="ego",
    mass=80,
    category=xosc.PedestrianCategory.pedestrian,
    boundingbox=xosc.BoundingBox(0.5, 0.5, 1.8, 0.0, 0.0, 0.9)
)

# Vehicle
vehicle = xosc.Vehicle(
    name="vehicle",
    vehicle_type=xosc.VehicleCategory.car,
    boundingbox=xosc.BoundingBox(
        width=2.0,
        length=4.5,
        height=1.5,
        x_center=0.0,
        y_center=0.0,
        z_center=0.75
    ),
    frontaxle=xosc.Axle(
        maxsteer=0.5,
        wheeldia=0.6,
        track_width=1.6,
        xpos=3.5,
        zpos=0.3
    ),
    rearaxle=xosc.Axle(
        maxsteer=0.0,
        wheeldia=0.6,
        track_width=1.6,
        xpos=0.8,
        zpos=0.3
    ),
    max_speed=30.0,
    max_acceleration=3.0,
    max_deceleration=6.0,
    mass=1500
)

entities.add_scenario_object("ego", ego)
entities.add_scenario_object("vehicle", vehicle)

# -----------------------------
# 2. INIT (teleport only)
# -----------------------------
init = xosc.Init()

# Place ego in VR start position
init.add_init_action(
    "ego",
    xosc.TeleportAction(
        xosc.LanePosition(
            s=10.0,
            offset=0.0,
            lane_id=0,
            road_id="221"
        )
    )
)

# Vehicle start position + speed
init.add_init_action(
    "vehicle",
    xosc.TeleportAction(
        xosc.LanePosition(
            s=50.0,
            offset=0.0,
            lane_id=-1,
            road_id="221"
        )
    )
)

init.add_init_action(
    "vehicle",
    xosc.AbsoluteSpeedAction(
        12.0,
        xosc.TransitionDynamics(
            xosc.DynamicsShapes.step,
            xosc.DynamicsDimension.time,
            0
        )
    )
)

# -----------------------------
# 3. VEHICLE TRAJECTORY
# -----------------------------
vehicle_traj = xosc.Trajectory("vehicle_traj", closed=False)
vehicle_positions = [
    xosc.LanePosition(50.0, 0.0, -1, "221"),
    xosc.LanePosition(60.0, 0.0, -1, "221"),
    xosc.LanePosition(70.0, 0.0, -1, "221"),
    xosc.LanePosition(80.0, 0.0, -1, "221"),
    xosc.LanePosition(90.0, 0.0, -1, "221"),
]
vehicle_times = [0, 5, 10, 15, 20]
veh_poly = xosc.Polyline(time=vehicle_times, positions=vehicle_positions)
vehicle_traj.add_shape(veh_poly)

# -----------------------------
# 4. VEHICLE EVENTS
# -----------------------------
vehicle_event = xosc.Event("vehicle_drive", xosc.Priority.overwrite)
vehicle_event.add_action(
    "follow_vehicle_traj",
    xosc.FollowTrajectoryAction(vehicle_traj, xosc.FollowingMode.position)
)
vehicle_event.add_trigger(
    xosc.ValueTrigger(
        "vehicle_start",
        0,
        xosc.ConditionEdge.rising,
        xosc.SimulationTimeCondition(0, xosc.Rule.greaterThan)
    )
)

# -----------------------------
# 5. MANEUVERS / GROUPS
# -----------------------------
ego_maneuver = xosc.Maneuver("ego_maneuver")

vehicle_maneuver = xosc.Maneuver("vehicle_maneuver")
vehicle_maneuver.add_event(vehicle_event)

ego_group = xosc.ManeuverGroup("ego_group")
ego_group.add_actor("ego")
ego_group.add_maneuver(ego_maneuver)

vehicle_group = xosc.ManeuverGroup("vehicle_group")
vehicle_group.add_actor("vehicle")
vehicle_group.add_maneuver(vehicle_maneuver)

# -----------------------------
# 6. STORY / ACT
# -----------------------------
act = xosc.Act(
    "main_act",
    xosc.ValueTrigger(
        "act_start",
        0,
        xosc.ConditionEdge.rising,
        xosc.SimulationTimeCondition(0, xosc.Rule.greaterThan)
    )
)
act.add_maneuver_group(ego_group)
act.add_maneuver_group(vehicle_group)

story = xosc.Story("urban_story")
story.add_act(act)

storyboard = xosc.StoryBoard(init)
storyboard.add_story(story)

# -----------------------------
# 7. VR TASKS (CustomCommandAction)
# -----------------------------
# Task 1: go to crosswalk
task1_event = xosc.Event("task_go_to_crosswalk", xosc.Priority.parallel)
task1_event.add_action(
    "task1_cmd",
    xosc.CustomCommandAction(
        type="VR_TASK",
        content='{"id":1,"task":"go","target":"crosswalk_main","instruction":"Walk to the crosswalk"}'
    )
)
task1_event.add_trigger(
    xosc.ValueTrigger("task1_start", 0, xosc.ConditionEdge.rising,
                      xosc.SimulationTimeCondition(0, xosc.Rule.greaterThan))
)

# Task 2: wait for green
task2_event = xosc.Event("task_wait_green", xosc.Priority.parallel)
task2_event.add_action(
    "task2_cmd",
    xosc.CustomCommandAction(
        type="VR_TASK",
        content='{"id":2,"task":"wait","target":"traffic_light_main","instruction":"Wait for green signal"}'
    )
)
task2_event.add_trigger(
    xosc.ValueTrigger("task2_start", 0, xosc.ConditionEdge.rising,
                      xosc.SimulationTimeCondition(5, xosc.Rule.greaterThan))
)

# Task 3: cross street
task3_event = xosc.Event("task_cross_street", xosc.Priority.parallel)
task3_event.add_action(
    "task3_cmd",
    xosc.CustomCommandAction(
        type="VR_TASK",
        content='{"id":3,"task":"cross","target":"opposite_sidewalk","instruction":"Cross the street safely"}'
    )
)
task3_event.add_trigger(
    xosc.ValueTrigger("task3_start", 0, xosc.ConditionEdge.rising,
                      xosc.SimulationTimeCondition(10, xosc.Rule.greaterThan))
)

# Add VR tasks to ego maneuver
ego_maneuver.add_event(task1_event)
ego_maneuver.add_event(task2_event)
ego_maneuver.add_event(task3_event)

# -----------------------------
# 8. ROAD NETWORK
# -----------------------------
road_network = xosc.RoadNetwork(roadfile="map.xodr")

# -----------------------------
# 9. SCENARIO
# -----------------------------
scenario = xosc.Scenario(
    name="UrbanPedestrianCrossing",
    author="Paritosh",
    parameters=xosc.ParameterDeclarations(),
    entities=entities,
    storyboard=storyboard,
    roadnetwork=road_network,
    catalog=xosc.Catalog()
)


\end{lstlisting}


\begin{verbatim}
\end{verbatim}

\subsection{Meeting Notes}


\subsubsection{06/01/2026}

\begin{itemize}
    \item \textbf{OSM Ontology tree:} Look for the complete OSM ontology tree.
    
    \item \textbf{Architecture diagram:} Add the complete architecture diagram of the system from OSM procedural generation to Scenario generation.
    
    \item \textbf{Finalise the ontology for extending the XOSC:} Finalize the ontology needed to extend the OpenScenario format. Focus on 5 categories - Static objects for interaction, Dynamic Objects  with no interaction, Dynamic objects with interactions, purely static objects and user-controlled.

    \item \textbf{Fix OSM generation:} Come up with a way to fix the orientations of the street furniture in OSM generations.

    \item \textbf{Use INRIA road network for reference:} Example, driving a car on the rue de lucioles.
\end{itemize}

\subsubsection{14/01/2026}

\begin{itemize}
    \item \textbf{Schema: } Add the OpenScenario schema tree in the doc for reference.
    \item \textbf{Focus on Navigation: } Start with navigation tasks from A to B for validation.
    \item \textbf{Extend OpenScenario DSL: } Extend the DSL Actions and entities to support navigation scenarios.
    \item \textbf{Pharmacy example: } Low vision patients tend to confuse pharmacies with green light, a scenario for that could be interesting.
    \item \textbf{Finalize the LLM part: } Finalize how LLM can take a functional description and create a valid open scenario from it.    
\end{itemize}

\subsubsection{04/02/2026}

\begin{itemize}
    \item \textbf{Scenarios: } Add the categories and examples of scenarios which have to be addressed for navigation.
    \item \textbf{Ontology: } Reference the added ontology with these scenarios
    \item \textbf{LLM: } Answer - "How and What we expect the LLM to generate this scenario?"
    \item \textbf{Prompts: } Add the prompts for the LLM.
    \item \textbf{Validation: } Revisit validation based on logical (example walking on un-navigable surfaces), semantic and compile errors.
\end{itemize}


\bibliographystyle{abbrv}
\bibliography{main}

\end{document}
